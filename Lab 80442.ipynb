{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"E:\\labs\\ML Lab\\ML Lab Spring 25\\timeseires\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from timeseires.utils.to_split import to_split\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from timeseires.utils.multivariate_single_step import multivariate_single_step\n",
    "from timeseires.utils.univariate_multi_step import univariate_multi_step\n",
    "from timeseires.utils.univariate_single_step import univariate_single_step\n",
    "from timeseires.utils.CosineAnnealingLRS import CosineAnnealingLRS\n",
    "from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookback = 24\n",
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    input_data = Input(shape=(time_steps, num_features))\n",
    "    x1 = Conv1D(16, 2, activation=\"relu\")(input_data)\n",
    "    x2 = Conv1D(16, 2, activation=\"relu\")(x1)\n",
    "    flatten = Flatten()(x2)\n",
    "    output_data = Dense(1)(flatten)\n",
    "    model = Model(input_data, output_data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24, 21)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 23, 16)            688       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 22, 16)            528       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 353       \n",
      "=================================================================\n",
      "Total params: 1,569\n",
      "Trainable params: 1,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = CNN()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "tensorflow.keras.utils.plot_model(model1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = r'E:\\labs\\ml lab\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = r'E:\\labs\\ml lab\\ML Lab\\lab8'\n",
    "#FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"\\history.png\"])\n",
    "#JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"\\history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define necessary paths and variables\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "FIG_PATH = os.path.sep.join([output_dir, \"training_plot.png\"])\n",
    "JSON_PATH = os.path.sep.join([output_dir, \"training_history.json\"])\n",
    "checkpoints = os.path.join(r'E:\\labs\\ml lab\\lab8', 'best_model.h5')\n",
    "\n",
    "start_epoch = 0  # Change this if you're resuming from a checkpoint\n",
    "\n",
    "# Construct the callback to save only the *best* model to disk based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                                   monitor=\"val_loss\",\n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1)\n",
    "\n",
    "# Make sure the TrainingMonitor class is implemented/imported properly\n",
    "TrainingMonitor1 = TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# Construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1, TrainingMonitor1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "# Define your model path\n",
    "model_path =r'E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5'\n",
    "\n",
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if not os.path.exists(model_path):  # If the model file doesn't exist, start fresh\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = CNN()  # define your CNN() model earlier\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss='mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(f\"[INFO] loading model from {model_path}...\")\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84907, 21), (24259, 21), (12130, 21))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path_dataset =r'E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\FYP DETAILS\\dada sets\\AEP\\AEP'\n",
    "path_tr = os.path.join(path_dataset, 'AEP_train.csv')\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values\n",
    "path_v = os.path.join(path_dataset, 'AEP_validation.csv')\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "path_te = os.path.join(path_dataset, 'AEP_test.csv')\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "\n",
    "path_scaler = os.path.join(path_dataset, 'AEP_Scaler.pkl')\n",
    "scaler         = pickle.load(open(path_scaler, 'rb'))\n",
    "\n",
    "train_set.shape, validation_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 11.499531507492065 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=0,target_len=1)\n",
    "validation_X, validation_y = univariate_multi_step(validation_set, time_steps, target_col=0,target_len=1)\n",
    "test_X, test_y = univariate_multi_step(test_set, time_steps, target_col=0,target_len=1)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0194 - mae: 0.0194 - mape: 1177.2150\n",
      "Epoch 00001: val_loss improved from inf to 0.01625, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 40s 15ms/step - loss: 0.0194 - mae: 0.0194 - mape: 1177.2150 - val_loss: 0.0163 - val_mae: 0.0163 - val_mape: 7.6260\n",
      "Epoch 2/10\n",
      "2650/2653 [============================>.] - ETA: 0s - loss: 0.0148 - mae: 0.0148 - mape: 656.6852\n",
      "Epoch 00002: val_loss improved from 0.01625 to 0.01446, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 41s 15ms/step - loss: 0.0148 - mae: 0.0148 - mape: 656.0554 - val_loss: 0.0145 - val_mae: 0.0145 - val_mape: 7.7883\n",
      "Epoch 3/10\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0127 - mape: 665.2752\n",
      "Epoch 00003: val_loss improved from 0.01446 to 0.01080, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 39s 15ms/step - loss: 0.0127 - mae: 0.0127 - mape: 665.1348 - val_loss: 0.0108 - val_mae: 0.0108 - val_mape: 5.7777\n",
      "Epoch 4/10\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0117 - mae: 0.0117 - mape: 528.3927\n",
      "Epoch 00004: val_loss improved from 0.01080 to 0.00965, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 48s 18ms/step - loss: 0.0117 - mae: 0.0117 - mape: 528.3927 - val_loss: 0.0096 - val_mae: 0.0096 - val_mape: 5.4823\n",
      "Epoch 5/10\n",
      "2651/2653 [============================>.] - ETA: 0s - loss: 0.0109 - mae: 0.0109 - mape: 373.4412\n",
      "Epoch 00005: val_loss did not improve from 0.00965\n",
      "2653/2653 [==============================] - 47s 18ms/step - loss: 0.0109 - mae: 0.0109 - mape: 373.2228 - val_loss: 0.0109 - val_mae: 0.0109 - val_mape: 6.4021\n",
      "Epoch 6/10\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0103 - mae: 0.0103 - mape: 493.7922\n",
      "Epoch 00006: val_loss improved from 0.00965 to 0.00957, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 51s 19ms/step - loss: 0.0103 - mae: 0.0103 - mape: 493.6879 - val_loss: 0.0096 - val_mae: 0.0096 - val_mape: 5.8003\n",
      "Epoch 7/10\n",
      "2653/2653 [==============================] - ETA: 0s - loss: 0.0098 - mae: 0.0098 - mape: 347.0411\n",
      "Epoch 00007: val_loss improved from 0.00957 to 0.00874, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 50s 19ms/step - loss: 0.0098 - mae: 0.0098 - mape: 347.0411 - val_loss: 0.0087 - val_mae: 0.0087 - val_mape: 5.5101\n",
      "Epoch 8/10\n",
      "2651/2653 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0095 - mape: 405.2386\n",
      "Epoch 00008: val_loss did not improve from 0.00874\n",
      "2653/2653 [==============================] - 54s 20ms/step - loss: 0.0095 - mae: 0.0095 - mape: 405.0013 - val_loss: 0.0097 - val_mae: 0.0097 - val_mape: 4.9087\n",
      "Epoch 9/10\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0093 - mape: 188.3166\n",
      "Epoch 00009: val_loss improved from 0.00874 to 0.00840, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 53s 20ms/step - loss: 0.0093 - mae: 0.0093 - mape: 188.2771 - val_loss: 0.0084 - val_mae: 0.0084 - val_mape: 4.6985\n",
      "Epoch 10/10\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0089 - mape: 191.0384\n",
      "Epoch 00010: val_loss improved from 0.00840 to 0.00820, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5\n",
      "2653/2653 [==============================] - 68s 26ms/step - loss: 0.0089 - mae: 0.0089 - mape: 190.9986 - val_loss: 0.0082 - val_mae: 0.0082 - val_mape: 3.7562\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 125.42\n",
      "Median Absolute Error (MedAE): 100.79\n",
      "Mean Squared Error (MSE): 26815.19\n",
      "Root Mean Squared Error (RMSE): 163.75\n",
      "Mean Absolute Percentage Error (MAPE): 0.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.69 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(r'E:\\labs\\ml lab\\lab8\\best_model.h5')\n",
    "\n",
    "y_pred_scaled   = model.predict(test_X)\n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled)) \n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = r'E:\\labs\\ml lab\\lab8\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model=r'E:\\labs\\ml lab\\lab8\\lab8\\best_model.h5'\n",
    "start_epoch= 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\best_model.h5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]\n",
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=24, num_features=21, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0070 - mean_absolute_error: 0.0070 - mean_absolute_percentage_error: 165.4842\n",
      "Epoch 00001: val_loss improved from inf to 0.00705, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\E2-cp-0001-loss0.01.h5\n",
      "2653/2653 [==============================] - 45s 17ms/step - loss: 0.0070 - mean_absolute_error: 0.0070 - mean_absolute_percentage_error: 165.4495 - val_loss: 0.0070 - val_mean_absolute_error: 0.0070 - val_mean_absolute_percentage_error: 3.1621\n",
      "Epoch 2/5\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0069 - mean_absolute_error: 0.0069 - mean_absolute_percentage_error: 105.0119 ETA: 0s - loss: 0.0069 - mean_absolute_error: 0.0069 - mean_absol\n",
      "Epoch 00002: val_loss did not improve from 0.00705\n",
      "2653/2653 [==============================] - 39s 15ms/step - loss: 0.0069 - mean_absolute_error: 0.0069 - mean_absolute_percentage_error: 104.9902 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073 - val_mean_absolute_percentage_error: 3.2729\n",
      "Epoch 3/5\n",
      "2650/2653 [============================>.] - ETA: 0s - loss: 0.0069 - mean_absolute_error: 0.0069 - mean_absolute_percentage_error: 74.3692\n",
      "Epoch 00003: val_loss improved from 0.00705 to 0.00694, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\E2-cp-0003-loss0.01.h5\n",
      "2653/2653 [==============================] - 39s 15ms/step - loss: 0.0069 - mean_absolute_error: 0.0069 - mean_absolute_percentage_error: 74.2992 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069 - val_mean_absolute_percentage_error: 3.1326\n",
      "Epoch 4/5\n",
      "2652/2653 [============================>.] - ETA: 0s - loss: 0.0068 - mean_absolute_error: 0.0068 - mean_absolute_percentage_error: 14.0563\n",
      "Epoch 00004: val_loss improved from 0.00694 to 0.00692, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\E2-cp-0004-loss0.01.h5\n",
      "2653/2653 [==============================] - 39s 15ms/step - loss: 0.0068 - mean_absolute_error: 0.0068 - mean_absolute_percentage_error: 14.0539 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069 - val_mean_absolute_percentage_error: 3.1648\n",
      "Epoch 5/5\n",
      "2651/2653 [============================>.] - ETA: 0s - loss: 0.0068 - mean_absolute_error: 0.0068 - mean_absolute_percentage_error: 18.4761\n",
      "Epoch 00005: val_loss improved from 0.00692 to 0.00669, saving model to E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\E2-cp-0005-loss0.01.h5\n",
      "2653/2653 [==============================] - 48s 18ms/step - loss: 0.0068 - mean_absolute_error: 0.0068 - mean_absolute_percentage_error: 18.4662 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067 - val_mean_absolute_percentage_error: 3.0355\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 109.03\n",
      "Median Absolute Error (MedAE): 86.94\n",
      "Mean Squared Error (MSE): 20713.59\n",
      "Root Mean Squared Error (RMSE): 143.92\n",
      "Mean Absolute Percentage Error (MAPE): 0.75 %\n",
      "Median Absolute Percentage Error (MDAPE): 0.6 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12105, 1)\n",
      "y_pred.shape=  (12105, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(r'E:\\ELECTRICAL ENGINEERIGH(COMNICATN)\\Semester-8th\\ML Lab\\lab8\\E2-cp-0005-loss0.01.h5')\n",
    "\n",
    "y_pred_scaled   = model.predict(test_X)\n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled)) \n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
